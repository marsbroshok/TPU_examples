{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YX1hxqUQn47M"
   },
   "source": [
    "## PyTorch/XLA TPU Profiling Colab tutorial\n",
    "\n",
    "*Note*: Since we're not using GCS in this tutorial, TPU side traces won't be collected. To collect full TPU traces follow [this tutorial](https://cloud.google.com/tpu/docs/pytorch-xla-performance-profiling-tpu-vm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLQPoJ6Fn8wF"
   },
   "source": [
    "### Install compatible PyTorch/XLA wheels and dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b3rCVMRazoeB",
    "tags": []
   },
   "source": [
    "### Use Colab Cloud TPU\n",
    "\n",
    "\n",
    "* On the main menu, click Runtime and select **Change runtime type**. Set \"TPU\" as the hardware accelerator.\n",
    "* The cell below makes sure you have access to a TPU on Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3P6b3uqfzpDI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CHzziBW5AoZH"
   },
   "source": [
    "#### Installing PyTorch/XLA\n",
    "\n",
    "Run the following cell (or copy it into your own notebook!) to install PyTorch, Torchvision, and PyTorch/XLA. It will take a couple minutes to run.\n",
    "\n",
    "The PyTorch/XLA package lets PyTorch connect to Cloud TPUs. (It's named PyTorch/XLA, not PyTorch/TPU, because XLA is the name of the TPU compiler.) In particular, PyTorch/XLA makes TPU cores available as PyTorch devices. This lets PyTorch create and manipulate tensors on TPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O53lrJMDn9Rd"
   },
   "outputs": [],
   "source": [
    "# !pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfSCdVlA8jFg"
   },
   "source": [
    "#### If you're using GPU with this colab notebook, run the below commented code to install GPU compatible PyTorch wheel and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1Vfg-rH8bF4"
   },
   "outputs": [],
   "source": [
    "#!pip install cloud-tpu-client==0.10 torch==1.13.0 https://storage.googleapis.com/tpu-pytorch/wheels/cuda/112/torch_xla-1.13-cp38-cp38-linux_x86_64.whl --force-reinstall "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPrij_iPfqTV"
   },
   "source": [
    "#### Only run the below commented cell if you would like a nightly release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJZrkoejQhxK"
   },
   "outputs": [],
   "source": [
    "# VERSION = \"1.13\"  #@param [\"1.13\", \"nightly\", \"20220315\"]  # or YYYYMMDD format\n",
    "# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "# !python pytorch-xla-env-setup.py --version $VERSION\n",
    "# import os \n",
    "# os.environ['LD_LIBRARY_PATH']='/usr/local/lib'\n",
    "# !echo $LD_LIBRARY_PATH\n",
    "\n",
    "# !sudo ln -s /usr/local/lib/libmkl_intel_lp64.so /usr/local/lib/libmkl_intel_lp64.so.1\n",
    "# !sudo ln -s /usr/local/lib/libmkl_intel_thread.so /usr/local/lib/libmkl_intel_thread.so.1\n",
    "# !sudo ln -s /usr/local/lib/libmkl_core.so /usr/local/lib/libmkl_core.so.1\n",
    "\n",
    "# !ldconfig\n",
    "# !ldd /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch.so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b3rCVMRazoeB",
    "tags": []
   },
   "source": [
    "### Use Cloud TPU VM\n",
    "\n",
    "* If this notebook is run inside the Cloud TPU VM, run the config below.\n",
    "* How to (create a Cloud TPU VM)[https://cloud.google.com/tpu/docs/managing-tpus-tpu-vm#tpu-vms] in advance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
    "os.environ[\"TPU_NAME\"] = \"dummy\"\n",
    "os.environ[\"XRT_TPU_CONFIG\"]=\"localservice;0;localhost:51011\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b3rCVMRazoeB",
    "tags": []
   },
   "source": [
    "### Use Cloud TPU Node\n",
    "\n",
    "* If this notebook is run on the Vertex AI Workbecnh, then you need to [create a Cloud TPU Node](https://cloud.google.com/tpu/docs/managing-tpus-tpu-vm#tpu-nodes) in advance.\n",
    "* The cell below makes sure you are connected to a Cloud TPU Node on GCP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TPU_NAME\"] = \"auv-tpu-node\"\n",
    "os.environ[\"TPU_IP_ADDRESS\"] = \"10.107.30.42\"\n",
    "os.environ[\"XRT_TPU_CONFIG\"] = f\"tpu_worker;0;{os.environ['TPU_IP_ADDRESS']}:8470\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rroH9yiAn-XE"
   },
   "source": [
    "### Define Parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMdPRFXIn_jH"
   },
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "import os\n",
    "FLAGS = {}\n",
    "FLAGS['data_dir'] = \"/tmp/cifar\"\n",
    "FLAGS['batch_size'] = 128\n",
    "FLAGS['num_workers'] = 4\n",
    "FLAGS['learning_rate'] = 0.02\n",
    "FLAGS['momentum'] = 0.9\n",
    "FLAGS['num_epochs'] = 200\n",
    "FLAGS['num_cores'] = 8 if os.environ.get('TPU_NAME', None) else 1\n",
    "FLAGS['log_steps'] = 20\n",
    "FLAGS['metrics_debug'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EP5H63aViwJe"
   },
   "outputs": [],
   "source": [
    "# Setup profiler env var\n",
    "os.environ['XLA_HLO_DEBUG'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Micd3xZvoA-c"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.debug.metrics as met\n",
    "import torch_xla.debug.profiler as xp\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.utils.utils as xu\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "  expansion = 1\n",
    "\n",
    "  def __init__(self, in_planes, planes, stride=1):\n",
    "    super(BasicBlock, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(\n",
    "        in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(planes)\n",
    "    self.conv2 = nn.Conv2d(\n",
    "        planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "    self.shortcut = nn.Sequential()\n",
    "    if stride != 1 or in_planes != self.expansion * planes:\n",
    "      self.shortcut = nn.Sequential(\n",
    "          nn.Conv2d(\n",
    "              in_planes,\n",
    "              self.expansion * planes,\n",
    "              kernel_size=1,\n",
    "              stride=stride,\n",
    "              bias=False), nn.BatchNorm2d(self.expansion * planes))\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.bn1(self.conv1(x)))\n",
    "    out = self.bn2(self.conv2(out))\n",
    "    out += self.shortcut(x)\n",
    "    out = F.relu(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "  def __init__(self, block, num_blocks, num_classes=10):\n",
    "    super(ResNet, self).__init__()\n",
    "    self.in_planes = 64\n",
    "\n",
    "    self.conv1 = nn.Conv2d(\n",
    "        3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(64)\n",
    "    self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "    self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "    self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "    self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "    self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "  def _make_layer(self, block, planes, num_blocks, stride):\n",
    "    strides = [stride] + [1] * (num_blocks - 1)\n",
    "    layers = []\n",
    "    for stride in strides:\n",
    "      layers.append(block(self.in_planes, planes, stride))\n",
    "      self.in_planes = planes * block.expansion\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.bn1(self.conv1(x)))\n",
    "    out = self.layer1(out)\n",
    "    out = self.layer2(out)\n",
    "    out = self.layer3(out)\n",
    "    out = self.layer4(out)\n",
    "    out = F.avg_pool2d(out, 4)\n",
    "    out = torch.flatten(out, 1)\n",
    "    out = self.linear(out)\n",
    "    return F.log_softmax(out, dim=1)\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "  return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyBPVi71h7ug"
   },
   "source": [
    "In the following cell we define the training loops and most importantly add tracing annotations `xp.StepTrace` and `xp.Trace` to that we'll be able to inspect in our profiler traces view on Tensorboard. `xp.StepTrace` specifically should be annotated only once per step as it denotes a full step and is used to calculate the step time for the model and is displayed on Tensorboard profile summary page. The `xp.Trace` context manager annotation can be sprinkled around on whichever parts you want a more detailed timeline of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vMl96KLoCq8"
   },
   "outputs": [],
   "source": [
    "SERIAL_EXEC = xmp.MpSerialExecutor()\n",
    "# Only instantiate model weights once in memory.\n",
    "WRAPPED_MODEL = xmp.MpModelWrapper(ResNet18())\n",
    "\n",
    "def train_resnet18(training_started):\n",
    "  torch.manual_seed(1)\n",
    "\n",
    "  # We are using fake data here (not real CIFAR dataset).\n",
    "  train_dataset_len = 50000  # Number of example in CIFAR train set.\n",
    "  train_loader = xu.SampleGenerator(\n",
    "      data=(torch.zeros(FLAGS['batch_size'], 3, 32,\n",
    "                        32), torch.zeros(FLAGS['batch_size'],\n",
    "                                          dtype=torch.int64)),\n",
    "      sample_count=train_dataset_len // FLAGS['batch_size'] //\n",
    "      xm.xrt_world_size())\n",
    "  test_loader = xu.SampleGenerator(\n",
    "      data=(torch.zeros(FLAGS['batch_size'], 3, 32,\n",
    "                        32), torch.zeros(FLAGS['batch_size'],\n",
    "                                          dtype=torch.int64)),\n",
    "      sample_count=10000 // FLAGS['batch_size'] // xm.xrt_world_size())\n",
    "\n",
    "  # Get loss function, optimizer, and model\n",
    "  device = xm.xla_device()\n",
    "  model = WRAPPED_MODEL.to(device)\n",
    "  optimizer = optim.SGD(model.parameters(), lr=FLAGS['learning_rate'],\n",
    "                        momentum=FLAGS['momentum'], weight_decay=5e-4)\n",
    "  loss_fn = nn.NLLLoss()\n",
    "\n",
    "  server = xp.start_server(9012)\n",
    "\n",
    "  def train_loop_fn(loader):\n",
    "    tracker = xm.RateTracker()\n",
    "    model.train()\n",
    "    for x, (data, target) in enumerate(loader):\n",
    "      if x == 5:\n",
    "        training_started.set()\n",
    "      # Let's now profile the training step.\n",
    "      with xp.StepTrace('train_loop', step_num=x):\n",
    "        # This profiles the construction of the graph.\n",
    "        with xp.Trace('build_graph'):\n",
    "          optimizer.zero_grad()\n",
    "          output = model(data)\n",
    "          loss = loss_fn(output, target)\n",
    "          loss.backward()\n",
    "\n",
    "        xm.optimizer_step(optimizer)\n",
    "        tracker.add(FLAGS['batch_size'])\n",
    "        if x % FLAGS['log_steps'] == 0:\n",
    "          print('[xla:{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n",
    "              xm.get_ordinal(), x, loss.item(), tracker.rate(),\n",
    "              tracker.global_rate(), time.asctime()), flush=True)\n",
    "\n",
    "  def test_loop_fn(loader):\n",
    "    total_samples = 0\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    data, pred, target = None, None, None\n",
    "    for data, target in loader:\n",
    "      output = model(data)\n",
    "      pred = output.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "      total_samples += data.size()[0]\n",
    "\n",
    "    accuracy = 100.0 * correct / total_samples\n",
    "    print('[xla:{}] Accuracy={:.2f}%'.format(\n",
    "        xm.get_ordinal(), accuracy), flush=True)\n",
    "    return accuracy, data, pred, target\n",
    "\n",
    "  # Train and eval loops\n",
    "  accuracy = 0.0\n",
    "  data, pred, target = None, None, None\n",
    "  for epoch in range(1, FLAGS['num_epochs'] + 1):\n",
    "    para_loader = pl.ParallelLoader(train_loader, [device])\n",
    "    train_loop_fn(para_loader.per_device_loader(device))\n",
    "    xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
    "\n",
    "    para_loader = pl.ParallelLoader(test_loader, [device])\n",
    "    accuracy, data, pred, target  = test_loop_fn(para_loader.per_device_loader(device))\n",
    "    if FLAGS['metrics_debug']:\n",
    "      xm.master_print(met.metrics_report(), flush=True)\n",
    "\n",
    "  return accuracy, data, pred, target\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2nL4HmloEyl"
   },
   "outputs": [],
   "source": [
    "# Start training processes\n",
    "def _mp_fn(rank, flags, training_started):\n",
    "  global FLAGS\n",
    "  FLAGS = flags\n",
    "  torch.set_default_tensor_type('torch.FloatTensor')\n",
    "  accuracy, data, pred, target = train_resnet18(training_started)\n",
    "  if rank == 0:\n",
    "    # Retrieve tensors that are on TPU core 0 and plot.\n",
    "    plot_results(data.cpu(), pred.cpu(), target.cpu())\n",
    "\n",
    "def target_fn(training_started):\n",
    "  sys.stdout = open('training_logs.stdout', 'w')\n",
    "  sys.stderr = open('training_logs.stderr', 'w')\n",
    "  xmp.spawn(_mp_fn, args=(FLAGS, training_started,),\n",
    "            nprocs=FLAGS['num_cores'], start_method='fork')\n",
    "  \n",
    "training_started = multiprocessing.Event()\n",
    "p = multiprocessing.Process(target=target_fn, args=(training_started,))\n",
    "p.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcS58faWifX7"
   },
   "source": [
    "The following cell first waits for the training to start up and then subsequently traces both the client VM side (i.e., where the XLA graph is built and input pipeline is run) and the TPU device side (where the actual compilation and execution happens). However, note that since we're running on Colab and not using GCS in this tutorial, TPU side traces won't be collected. To collect full TPU traces follow this [tutorial](https://cloud.google.com/tpu/docs/pytorch-xla-performance-profiling-tpu-vm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a log dir on the GCS\n",
    "log_bucket = \"gs://auv-pytorch-tpu-profiling-logs\"\n",
    "!gsutil mb {log_bucket}\n",
    "\n",
    "print(\"Logs path: \", f'{log_bucket}/tmp/tensorboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zppUkQI3fv2p"
   },
   "outputs": [],
   "source": [
    "training_started.wait(120)\n",
    "\n",
    "# tpu_ip = os.environ.get('TPU_IP_ADDRESS')\n",
    "xp.trace('localhost:9012', f'{log_bucket}/tmp/tensorboard')  # client side profiling\n",
    "# xp.trace(f'{tpu_ip}:8466', f'{log_bucket}/tmp/tensorboard')  # need GCS bucket for all traces to be written"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the tensorboard server in the terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Efdo1gx4bYRY"
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir gs://auv-pytorch-tpu-profiling-logs/tmp/tensorboard --load_fast=false\n",
    "# Click on \"INACTIVE\" dropdown and select \"PROFILE\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "PyTorch/XLA Profling Colab Tutorial",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m106",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m106"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
