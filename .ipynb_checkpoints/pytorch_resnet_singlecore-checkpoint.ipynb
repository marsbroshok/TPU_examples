{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82Yd9t9H9cjt"
   },
   "source": [
    "### Single-core Training FashionMNIST on Cloud TPU\n",
    "\n",
    "In this notebook we explore the simple code changes necessary to train and evaluate a model on a TPU.\n",
    "\n",
    "Some of the some of the torch_xla functions we will use include:\n",
    "* `device = xm.xla_device()`\n",
    "* `xm.is_master_ordinal()`\n",
    "* `xm.optimizer_step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def trainer():\n",
    "    \n",
    "    # Random seed for initialization\n",
    "    torch.manual_seed(flags['seed'])\n",
    "    # Sets device to Cloud TPU core\n",
    "    device = xm.xla_device()\n",
    "\n",
    "    # Normalization for dataloader \n",
    "    # TorchVision models require RGB (3 x H x W) images\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "    to_rgb = transforms.Lambda(lambda image: image.convert('RGB'))\n",
    "    resize = transforms.Resize((224, 224))\n",
    "    my_transform = transforms.Compose([resize, to_rgb, transforms.ToTensor(), normalize])\n",
    "\n",
    "    # Checks if current process is the master ordinal (0)\n",
    "    # Other workers wait for master to complete download\n",
    "    if not xm.is_master_ordinal():\n",
    "        xm.rendezvous('download_only_once')\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(\n",
    "        \"/tmp/fashionmnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=my_transform)\n",
    "\n",
    "    test_dataset = datasets.FashionMNIST(\n",
    "        \"/tmp/fashionmnist\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=my_transform)\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_dataset)\n",
    "    test_sampler = torch.utils.data.RandomSampler(test_dataset)\n",
    "\n",
    "    # Dataloaders load data in batches\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=flags['batch_size'],\n",
    "        sampler=train_sampler\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=flags['batch_size'],\n",
    "        sampler=test_sampler\n",
    "    )\n",
    "\n",
    "    # Model, optimizer, and loss function \n",
    "    # We use a resnet18 model for the 10 classes of the FashionMNIST dataset\n",
    "    net = torchvision.models.resnet18(num_classes=10).to(device).train()\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "    def train():\n",
    "        train_start = time.time()\n",
    "        for data, targets in iter(train_loader):\n",
    "            # Sends data and targets to device\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # Get prediction\n",
    "            output = net(data)\n",
    "            # Loss function\n",
    "            loss = loss_fn(output, targets)\n",
    "            # Update model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # xm.optimizer_step(optimizer) consolidates the gradients between cores\n",
    "            # and issues the XLA device step computation.\n",
    "            xm.optimizer_step(optimizer, barrier=True)\n",
    "\n",
    "        elapsed_train_time = time.time() - train_start\n",
    "        print(f\"Finished training. Train time was: {elapsed_train_time}\")\n",
    "\n",
    "    def test():\n",
    "        net.eval()\n",
    "        eval_start = time.time()\n",
    "        with torch.no_grad():\n",
    "            num_correct = 0\n",
    "            total_guesses = 0\n",
    "\n",
    "        for data, targets in iter(test_loader):\n",
    "            # Sends data and targets to device\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = net(data)\n",
    "            best_guesses = torch.argmax(output, 1)\n",
    "            # Calculate accuracy\n",
    "            num_correct += torch.eq(targets, best_guesses).sum().item()\n",
    "            total_guesses += flags['batch_size']\n",
    "\n",
    "        accuracy = 100.0 * num_correct / total_guesses\n",
    "        elapsed_eval_time = time.time() - eval_start\n",
    "        print(f\"Finished evaluation. Evaluation time was: {elapsed_eval_time}\")\n",
    "        print(f\"Guessed {num_correct} of {total_guesses} correctly for {accuracy} % accuracy.\")\n",
    "\n",
    "        return accuracy, data, targets\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    data, targets = None, None\n",
    "    \n",
    "    # Loop through epochs, calling the train and eval functions above\n",
    "    for epoch in range(flags['num_epochs']):\n",
    "        tic = time.time()\n",
    "        train()\n",
    "        xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
    "        toc = time.time()\n",
    "        accuracy, data, targets  = test()\n",
    "        # Calculate training time per epoch\n",
    "        epoch_time = round(toc-tic, 2)\n",
    "        print(f\"Epoch trained on a single TPU in {epoch_time} seconds\")\n",
    "\n",
    "    return accuracy, data, targets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 991
    },
    "id": "YSKorOP29cts",
    "outputId": "79ca7502-25cf-41f3-eb35-b4f225e7f925"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "flags = {}\n",
    "flags['batch_size'] = 8\n",
    "flags['num_epochs'] = 1\n",
    "flags['seed'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Pytorch (Local)",
   "language": "python",
   "name": "local-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
